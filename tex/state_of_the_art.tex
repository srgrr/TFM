\section{State of The Art}
\label{sec:state_of_the_art}

\subsection{COMPSs}
\label{subsec:compss_state_of_the_art}
\input{compss.tex}

\subsection{Mare Nostrum IV}
\label{subsec:mare_nostrum}
%TODO: MENCIONAR QUE COMPSS ESTA PENSADO PARA SER USADO AQUI
Mare Nostrum is a generic name to refer to the main supercomputer of the Barcelona Supercomputing Center \footnote{https://www.bsc.es}. Mare Nostrum IV is the fourth version of this supercomputer.

\subsection{GPFS}
\label{subsec:gpfs}
GPFS \cite{schmuck2002gpfs} is a distributed file system developed by IBM. It gives a perceived behaviour of a regular POSIX file system, while guaranteeing consistency between different computational resources, and a correct parallel access to its files. Under this model, any node has access any file at any location. As we can see in figure \ref{fig:gpfs_schema} a node accesses data through a switching fabric. A switching fabric is a kind network topology in which any two nodes connect between each other through a series of switches. This topology allows a more efficient communication between nodes than other topologies such as broadcast networks.
\begin{figure}
\centering
\includegraphics{figures/gpfs_schema.png}
\caption{GPFS Shared disk environment. Figure 1 from \cite{schmuck2002gpfs}}
\label{fig:gpfs_schema}
\end{figure}
GPFS is available at the Mare Nostrum IV supercomputer, and COMPSs takes advantage of it by delegating the file system many tasks such as file transfers, consistency across computational resources, and so on. It also makes task scheduling easier, as the data locality factor can be ignored by the COMPSs scheduler, focusing only on load balancing. Being more specific, if COMPSs runs under a GPFS file system it will consider that any piece of data is available anywhere, instead of explicitly keeping track of its locations.

\subsection{Queue Systems - SLURM and LSF}
\label{subsec:hpc_queues}
Most supercomputers have many concurrent users. All of these users want to use some of the resources of the supercomputer, and usually in a selfish manner. This situation creates a lot of conflicts between users, and even some unethical behaviors such as some user killing the processes of other users. Also, many benchmarks and experiments require no noise introduced by concurrent, unrelated processes running in the same machine, so resource exclusivity must be guaranteed in these cases.\\
\\
The most common solution to the two aforementioned problems is to divide the different nodes of a supercomputer into login nodes and computing nodes. When a user opens a session in some supercomputer he will \textit{land} into some login node. Computing nodes are unreachable or even not visible by regular users, and the only way to have access to them is to ask the system for resources and wait until the system lends them to the user. The most common implementation of this resource assignment mechanism is a queue system. A queue system processes all the requests from the users, gives them a priority as a function of various parameters and lends them the requested resources according to these priorities, as a process scheduler does with processes in an operative system.\\
\\
Two of the most common queue systems are LSF \cite{zhou1992lsf} and SLURM \cite{yoo2003slurm}. All the experiments of this project will be done in the Mare Nostrum 4 supercomputer, which uses SLURM.\\
\\
Although SLURM has its own micro-language and instructions, such as \verb|srun|, and submissions scripts, most of the experiments done in this project will not need them, as we will have generic queueing scripts available to us. A generic queueing script is a script capable to work with various queue systems to generate the corresponding specific queueing scripts. In our case, our script will translate our orders into a bunch of \verb|srun| commands and similar.\\
