\section{Combining Storage Systems with COMPSs}
\label{sec:storage}
Most COMPSs objects are created by the user and managed by the Runtime. The data transferring software is a self-made library based on NIO \footnote{https://docs.oracle.com/javase/7/docs/api/java/nio/package-summary.html}. Although this is usually a good enough solution for most use cases, there are three scenarios in which it may be a disadvantage to use this library:

\begin{enumerate}
\item The objects are the output of some previous application
\item The outputs of the COMPSs application are the input of some other application
\item The filesystem and/or the network presents huge bandwidth limitations
\end{enumerate}

The idea of replacing files with databases helps with these three items. Custom storage systems can help the user to avoid dealing with the file system and execute complex, distributed workflows using only the RAM of all the machines. Another great advantage is that these storage systems are specialized, and therefore have better implementations than us, on transferring objects between nodes, so replacing COMPSs with them may give us a little performance boost.
%TODO: FINISH THIS CHAPTER

\subsection{Defining a Storage API}
\label{subsec:storage_api}
The need of defining a storage API comes from the fact that some research groups are interested in mixing some of their tools with the COMPSs Programming Model. The two main tools or products from these research groups are DataClay \cite{DataClay} and Hecuba \cite{alomar2015hecuba}.

After some meetings it was decided that storage systems should implement all the same API. This would make implementations simpler and would allow the users to run the same application with different storage implementations just by changing the API implementation in their configuration. All API implementations must define a \verb|StorageObject| class from which any object that interacts with the database must inherit. A list of these methods can be found in table \ref{table:storage_api_public_methods}.

\begin{table}[]
\centering
\begin{tabular}{|l|l|}
\hline
Method name                                                     & Description                                                        \\ \hline
\verb|make_persistent(id = None)| & Make the Storage Object persistent                                 \\ \hline
\verb|delet_persistent()|        & Delete the Storage Object from the Storage System                  \\ \hline
\verb|getID()|                     & Get the identifier of the Storage Object                           \\ \hline
\verb|getByID(*objects)| (static)  & Retrieve the objects with the given identifiers \\ \hline
\end{tabular}
\caption{Public methods of the storage API}
\label{table:storage_api_public_methods}
\end{table}

COMPSs can be configured to let it know that the user is using some storage system. In that case, a path with a \textit{implementation bundle} should be provided. COMPSs will add to the \verb|CLASSPATH| and \verb|PYTHONPATH| variables the necessary paths.\\
\\
The target storage system can be an already living and existing database, or can be created by COMPSs if a script to do that is specified. In the first case, a list with the addresses of the storage node(s) should be provided. Note that this list can refer to nodes that have no COMPSs worker instances running on it. This is ideal for COMPSs applications which take the outputs of some other applications as their inputs. In the second case a \verb|storage_init.sh| script should be provided. This script receives the list of the COMPSs nodes as command line arguments and it is responsible to create the corresponding storage backend. It is also advisable to implement a \verb|storage_stop.sh| script if the created storage system by COMPSs is not necessary after the COMPSs execution.

\subsection{A Practical Implementation: Redis}
\label{subsec:storage_redis}
The first step towards validating this storage API consisted of providing a valid, functional implementation of the previously defined API. For this purpose Redis was chosen.\\
\\
Redis \footnote{https://redis.io/} is a simple Key-Value distributed storage database. Redis can be seen as a distributed hash map with $2^14 = 16384$ slots. Each key is either chosen by the user or randomly assigned, and it determines the position of this object in the hash table. More precisely, given a key $k$, and a value $v$, $v$ will be stored at the position $\textrm{CRC16}(k) \mod \quad 16384$. CRC16\footnote{https://en.wikipedia.org/wiki/Cyclic\_redundancy\_check} is a known checksum-like method used by many devices and network protocols to check that a message has been sent with no errors, and it can also be used as a quick hash function.\\
\\
Our implementation serializes objects in-memory and stores them as byte arrays in the database. Although this does not save us from serializing objects it is enough to avoid us to deal with the filesystem, allowing us to do all the operations in-memory. Huge byte arrays are split in distributed blocks to avoid long-term load imbalances and to increase long-term data locality. If we have $N$ worker nodes, our data is $M$ bytes long, it is available at only one node, and nodes are busy enough to make data locality irrelevant then the expected data transferring time is
$$E_1[N] = \frac{N - 1}{N} \times M$$
If our data is split into $K$ blocks, $\frac{M}{K}$ bytes each block, and each block is at a different node then the expected data transferring time can expressed as

$$E_2[N] \leq \frac{K}{N}(K - 1)\frac{M}{K} + \frac{N - K}{N}M$$

By simplifying $M = 1$ we obtain that $E_2[N] \leq E_1[N] = \frac{N - 1}{N}$. However, this second approach offers the opportunity to introduce parallelism and, in fact, most practical applications show that the required time to transfer the $(K - 1)$ remaining blocks is usually less or equal than $2.5\frac{M}{K}$, or something around that, and that in the worst case scenario the introduced overhead is relatively negligible, which is consistent with the two formulas above. Another important detail is that Redis offers the possibility to have replicas. A replica is a Redis instance that mirrors the behavior of some other Redis instance and is usually located in a different node/machine than the original one. This reduces even more the expected transfer time, but it introduces a dangerous tradeoff between time and space.

Another detail about our Redis implementation is that Redis offers no direct way to make some piece of data be stored in some node. However, the Storage API defines a method which allows the user to do that. Our solution to this problem consisted of simply generating random keys until one of them mapped to a valid slot. This adds almost no overhead, as the number of nodes tends to be a very manageable number such as 16, 32, 64, or at most 128 in most practical cases.

\subsection{Practical Applications}
\label{subsec:storage_apps}
\subsubsection{K-Means}
\label{subsubsec:kmeans_redis}
K-Means \cite{Lloyd82leastsquares} is a clustering algorithm which, given $N$ $k$-dimensional points and an integer $c$, assigns each point a label between $1$ and $c$. The idea is that these labels represent groups of \textit{similar} points. An example of what this algorithm computes can be found in figure \ref{fig:kmeans_example}.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/kmeans_example.png}
\caption{A set of points grouped by the K-Means algorithm. Black points represent centroids, colours represent different groups}
\label{fig:kmeans_example}
\end{figure}

The algorithm can be sumarized as follows:

\begin{enumerate}
\item Generate $c$ random $d$ dimensional points, call them \textit{centroids}
\item For each point of the input data, compute the nearest centroid, assign them labels according to which centroid is the closest
\item For each group, compute the mean of its members. Use this mean point as the new centroid
\item Repeat step 2 until the new centroids are \textit{equal enough} to the old ones
\end{enumerate}

This algorithm can be easily run in a distributed environment by dividing the input points into chunks and replicating the centroids in each computing node. Note that the input points will not vary during all the execution, and that the centroids usually represent a very small amount of data, so no big network transfers should be expected here, and therefore no huge improvements should be observed with the storage implementation. Our PyCOMPSs implementation is the following can be found in appendix \ref{subsec:kmeans_redis}.

Some results of how this application scales and behaves with various, different storage implementations and with files can be found in figures \ref{fig:kmeans_storage_dep_graph} \ref{fig:kmeans_strong_redis} \ref{fig:kmeans_strong_speedup_redis}, \ref{fig:kmeans_weak_redis}, and \ref{fig:kmeans_weak_speedup_redis}.

\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.3]{figures/kmeans_storage_dep_graph.png}
\caption{Dependency graph of a 6-iteration K-Means execution with $4$ point fragments.}
\label{fig:kmeans_storage_dep_graph}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.5]{figures/storage/kmeans_strong.png}
\caption{Strong scaling graph of our various storage implementations}
\label{fig:kmeans_strong_redis}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.5]{figures/storage/kmeans_strong_speedup.png}
\caption{Strong scaling speedup graph of our various storage implementations}
\label{fig:kmeans_strong_speedup_redis}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.5]{figures/storage/kmeans_weak.png}
\caption{Weak scaling graph of our various storage implementations}
\label{fig:kmeans_weak_redis}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.5]{figures/storage/kmeans_weak_speedup.png}
\caption{Weak scaling speedup graph of our various storage implementations}
\label{fig:kmeans_weak_speedup_redis}
\end{figure}

As we can see, our storage implementation does not improve the overall performance of this application. However, as we can see in figure \ref{fig:kmeans_transfer_trace}, this applications has little to no heavy transfers, only at the beggining, so these results are more or less expected.


%TODO: GET THE KMEANS TRACE

\subsubsection{Matrix Multiplication}
\label{subsubsec:matmul_redis}
The matrix multiplication is a very common algorithm and it is usually the preferred example of what an embarrassingly parallel application is (i.e: a parallel application with no dependencies). Its distributed version is also interesting but due to other property: the enormous amount of required data transfers. Let's take a look to the following code:

\inputminted{python}{snippets/matmul_python.py}

This code can be parallelized in any of the three loops. The only special requirement is to make sure that no pair of additions over the same $C_{i, j}$ are done concurrently. Our COMPSs implementation can be found in appendix \ref{subsec:matmul_redis}. As we can see, we \textit{solve} the problem by arranging all the tasks that involve some $C_{i, j}$ in a dependency chain.\\
\\
As we can see in figures \ref{fig:matmul_strong_redis} and \ref{fig:matmul_strong_speedup_redis} the Matrix Multiplication algorithm gets a huge benefit from our storage systems. This is expected, as this application is a great example of a very transfer-intensive workflow.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/storage/matmul.png}
\caption{Strong scaling graph of our various storage implementations}
\label{fig:matmul_strong_redis}
\end{figure}



\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/storage/matmul_speedup.png}
\caption{Strong scaling speedup graph of our various storage implementations}
\label{fig:matmul_strong_speedup_redis}
\end{figure}

