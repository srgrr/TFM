\chapter{Background}
\label{sec:state_of_the_art}

%\section{Mare Nostrum IV}
%\label{subsec:mare_nostrum}
%Mare Nostrum is a generic name to refer to the main supercomputer of the Barcelona Supercomputing Center \footnote{https://www.bsc.es}. Mare Nostrum IV is the fourth version of this supercomputer. 

\section{COMPSs}
\label{subsec:compss_state_of_the_art}
\input{compss.tex}

\newpage

\section{GPFS}
\label{subsec:gpfs}
GPFS \cite{schmuck2002gpfs} is a distributed file system developed by IBM. It gives a perceived behaviour of a regular POSIX file system, while guaranteeing consistency between different computational resources, and a correct parallel access to its files. Under this model, any node has access any file at any location. As we can see in figure \ref{fig:gpfs_schema} a node accesses data through a switching fabric. A switching fabric is a kind network topology in which any two nodes connect between each other through a series of switches. This topology allows a more efficient communication between nodes than other topologies such as broadcast networks.
\begin{figure}[ht!]
\centering
\includegraphics{figures/gpfs_schema.png}
\caption{GPFS Shared disk environment. Figure 1 from \cite{schmuck2002gpfs}}
\label{fig:gpfs_schema}
\end{figure}
GPFS is available at the Mare Nostrum IV supercomputer, and COMPSs takes advantage of it by delegating the file system many tasks such as file transfers, consistency across computational resources, and so on. It also makes task scheduling easier, as the data locality factor can be ignored by the COMPSs scheduler, focusing only on load balancing. Being more specific, if COMPSs runs under a GPFS file system it will consider that any piece of data is available anywhere, instead of explicitly keeping track of its locations.

\section{Queue Systems - SLURM and LSF}
\label{subsec:hpc_queues}
Most supercomputers have many concurrent users. All of these users want to use some of the resources of the supercomputer, and usually in a selfish manner. This situation creates a lot of conflicts between users, and even some unethical behaviors such as some user killing the processes of other users. Also, many benchmarks and experiments require no noise introduced by concurrent, unrelated processes running in the same machine, so resource exclusivity must be guaranteed in these cases.\\
\\
The most common solution to the two aforementioned problems is to divide the different nodes of a supercomputer into login nodes and computing nodes. When a user opens a session in some supercomputer he will \textit{land} into some login node. Computing nodes are unreachable or even not visible by regular users, and the only way to have access to them is to ask the system for resources and wait until the system lends them to the user. The most common implementation of this resource assignment mechanism is a queue system. A queue system processes all the requests from the users, gives them a priority as a function of various parameters and lends them the requested resources according to these priorities, as a process scheduler does with processes in an operative system.\\
\\
Two of the most common queue systems are LSF \cite{zhou1992lsf} and SLURM \cite{yoo2003slurm}. All the experiments of this project will be done in the Mare Nostrum 4 supercomputer, which uses SLURM.\\
\\
Although SLURM has its own micro-language and instructions, such as \verb|srun|, and submissions scripts, most of the experiments done in this project will not need them, as we will have generic queueing scripts available to us. A generic queueing script is a script capable to work with various queue systems to generate the corresponding specific queueing scripts. In our case, our script will translate our orders into a bunch of \verb|srun| commands and similar.\\

\section{Extrae and Paraver}
Extrae \footnote{https://tools.bsc.es/doc/pdf/extrae.pdf} and Paraver \cite{paraver} are two profiling tools developed at the BSC. Extrae is an instrumentation software to trace programs. A program instrumented with Extrae usually emits events. An event usually consists of an identifier or label, and a timestamp indicating the exact moment of its emission (according to the clock of the machine that executed the program). These events can be later visualized with Paraver as what is known as a \textit{trace}. An example of a trace can be found in figure \ref{fig:trace_example}.

\begin{figure}[ht!]
\centering
\includegraphics[scale = 0.3]{figures/matmul_trace.png}
\caption{An example of a trace. Each row represents a thread (or a process depending on the case), colored segments are different tasks executed by the thread, and yellow lines are network transfers between different computing nodes. Time is represented as the horizontal axis, from left to right.}
\label{fig:trace_example}
\end{figure}

Traces can be visualized in many different ways, depending on the needs of the user. This project will only use traces like the one from figure \ref{fig:trace_example}, and any relevant information about traces will be mentioned in the caption of the corresponding figure.\\
\\
In this project no explicit use of Extrae will appear, as we will work with a framework which already has Extrae instrumentation.

\section{Hecuba and DataClay}
\label{sec:hecuba}
Hecuba \cite{alomar2015hecuba} is a distributed non-relational database. It implements a runtime which coordinates various independent databases to make them work as a cluster, and provides a set of functions to allow the user make queries to this cluster.\\
\\
In a Hecuba cluster each instance is able to accept and to compute queries, allowing the user to make all his queries to the nearest node without needing to worry about the underlying topology of the cluster.\\
\\
DataClay \cite{DataClay} is another distributed database implementation. In a DataClay cluster, each individual instance is a Postgres SQL database to which the user has no direct access. Instead, DataClay implements a series of methods and functions to interact with the whole cluster.\\
\\
Both storage implementations follow the Storage Object model. A Storage Object is an instance of an OOP language class which has some of its attributes marked as class fields. A class field is automatically accessed and synchronized with the storage backend. This paradigm allows the user to avoid any kind of direct query and to have any explicit database knowledge to interact with the storage backend.\\
\\
Hecuba supports data partitioning via the special \textit{split} method. If a piece of data with key $k$ is split and queried then an iterator is returned instead. An iterator is just a key and a reference to another iterator, like a linked list. As we can see in figure \ref{fig:hecuba_cluster}, this means that a piece of data can be split among all the nodes of a cluster, and thus improving the balancing.
\begin{figure}
    \centering
    \includegraphics[scale = 0.25]{figures/HecubaIterators.png}
    \caption{Graphic description of a Hecuba cluster. Source: \cite{alomar2015hecuba}}
    \label{fig:hecuba_cluster}
\end{figure}
\newpage